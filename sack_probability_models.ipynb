{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jskaza/nfl-big-data-bowl-2023/blob/master/sack_probability_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ze7qdhqi4Pk"
      },
      "source": [
        "# Using ReSaP and ReHaP to Predict Pass Rusher Impact as Plays Develop\n",
        "*ReSaP: **Re**current **Sa**ck **P**robabilities*\n",
        "\n",
        "*ReHaP: **Re**current **Ha**voc **P**robabilities*\n",
        "\n",
        "**Jon Skaza & Matt Guthrie**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tFSzTpcXK_m",
        "outputId": "429581d1-38e2-4805-ed06-f12f6bbbda21",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  in_colab = True\n",
        "except:\n",
        "  in_colab = False\n",
        "seed = 314 # for reproducibility, used in various places"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LWTFl3HYaciz"
      },
      "source": [
        "## Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "Ox-a-y6_XK_r",
        "outputId": "0a4150ab-2b1d-4839-c155-bab7abcd3b80",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>game_id</th>\n",
              "      <th>play_id</th>\n",
              "      <th>nfl_id</th>\n",
              "      <th>speed</th>\n",
              "      <th>pff_sack</th>\n",
              "      <th>havoc</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>dist_from_qb</th>\n",
              "      <th>qb_in_tackle_box</th>\n",
              "      <th>...</th>\n",
              "      <th>receiver_sep_1</th>\n",
              "      <th>receiver_sep_2</th>\n",
              "      <th>receiver_sep_3</th>\n",
              "      <th>receiver_sep_4</th>\n",
              "      <th>receiver_sep_5</th>\n",
              "      <th>quarter</th>\n",
              "      <th>down</th>\n",
              "      <th>yards_to_go</th>\n",
              "      <th>absolute_yardline_number</th>\n",
              "      <th>score_delta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021090900</td>\n",
              "      <td>97</td>\n",
              "      <td>41263</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.74</td>\n",
              "      <td>-5.03</td>\n",
              "      <td>7.802083</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.030017</td>\n",
              "      <td>2.706917</td>\n",
              "      <td>6.139422</td>\n",
              "      <td>1.377679</td>\n",
              "      <td>4.278247</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021090900</td>\n",
              "      <td>97</td>\n",
              "      <td>41263</td>\n",
              "      <td>1.08</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.63</td>\n",
              "      <td>-5.01</td>\n",
              "      <td>7.766557</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.961689</td>\n",
              "      <td>2.659568</td>\n",
              "      <td>6.040149</td>\n",
              "      <td>1.369708</td>\n",
              "      <td>4.222345</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021090900</td>\n",
              "      <td>97</td>\n",
              "      <td>41263</td>\n",
              "      <td>1.30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.47</td>\n",
              "      <td>-4.99</td>\n",
              "      <td>7.695193</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.859266</td>\n",
              "      <td>2.607221</td>\n",
              "      <td>5.928642</td>\n",
              "      <td>1.388416</td>\n",
              "      <td>3.898166</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021090900</td>\n",
              "      <td>97</td>\n",
              "      <td>41263</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-4.94</td>\n",
              "      <td>7.603138</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.719577</td>\n",
              "      <td>2.452305</td>\n",
              "      <td>5.756813</td>\n",
              "      <td>1.424430</td>\n",
              "      <td>3.516049</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2021090900</td>\n",
              "      <td>97</td>\n",
              "      <td>41263</td>\n",
              "      <td>2.16</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.04</td>\n",
              "      <td>-4.83</td>\n",
              "      <td>7.404627</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.612279</td>\n",
              "      <td>2.297325</td>\n",
              "      <td>5.472961</td>\n",
              "      <td>1.480034</td>\n",
              "      <td>3.040066</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 43 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      game_id  play_id  nfl_id  speed  pff_sack  havoc     x     y  \\\n",
              "1  2021090900       97   41263   0.96         0      1  1.74 -5.03   \n",
              "2  2021090900       97   41263   1.08         0      1  1.63 -5.01   \n",
              "3  2021090900       97   41263   1.30         0      1  1.47 -4.99   \n",
              "4  2021090900       97   41263   1.48         0      1  1.31 -4.94   \n",
              "5  2021090900       97   41263   2.16         0      1  1.04 -4.83   \n",
              "\n",
              "   dist_from_qb  qb_in_tackle_box  ...  receiver_sep_1  receiver_sep_2  \\\n",
              "1      7.802083               1.0  ...        3.030017        2.706917   \n",
              "2      7.766557               1.0  ...        2.961689        2.659568   \n",
              "3      7.695193               1.0  ...        2.859266        2.607221   \n",
              "4      7.603138               1.0  ...        2.719577        2.452305   \n",
              "5      7.404627               1.0  ...        2.612279        2.297325   \n",
              "\n",
              "   receiver_sep_3  receiver_sep_4  receiver_sep_5  quarter  down  yards_to_go  \\\n",
              "1        6.139422        1.377679        4.278247        1     3            2   \n",
              "2        6.040149        1.369708        4.222345        1     3            2   \n",
              "3        5.928642        1.388416        3.898166        1     3            2   \n",
              "4        5.756813        1.424430        3.516049        1     3            2   \n",
              "5        5.472961        1.480034        3.040066        1     3            2   \n",
              "\n",
              "   absolute_yardline_number  score_delta  \n",
              "1                      43.0            0  \n",
              "2                      43.0            0  \n",
              "3                      43.0            0  \n",
              "4                      43.0            0  \n",
              "5                      43.0            0  \n",
              "\n",
              "[5 rows x 43 columns]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if in_colab:\n",
        "  drive.mount(\"/content/drive\")\n",
        "  path = \"/content/drive/MyDrive/nfl-big-data-bowl-2023\"\n",
        "else:\n",
        "  path = os.environ.get(\"BIG_DATA_BOWL\")\n",
        "\n",
        "df = pd.read_csv(f\"{path}/data/dataset.csv\", index_col=0).head(30000)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOD27h-Way_5",
        "outputId": "f14d46b7-7f7d-4c13-a633-015bb8ff4551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5000 entries, 1 to 5000\n",
            "Data columns (total 43 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   game_id                   5000 non-null   int64  \n",
            " 1   play_id                   5000 non-null   int64  \n",
            " 2   nfl_id                    5000 non-null   int64  \n",
            " 3   speed                     5000 non-null   float64\n",
            " 4   pff_sack                  5000 non-null   int64  \n",
            " 5   havoc                     5000 non-null   int64  \n",
            " 6   x                         5000 non-null   float64\n",
            " 7   y                         5000 non-null   float64\n",
            " 8   dist_from_qb              5000 non-null   float64\n",
            " 9   qb_in_tackle_box          5000 non-null   float64\n",
            " 10  x_blocker_1               5000 non-null   float64\n",
            " 11  x_blocker_2               5000 non-null   float64\n",
            " 12  x_blocker_3               5000 non-null   float64\n",
            " 13  x_blocker_4               5000 non-null   float64\n",
            " 14  x_blocker_5               5000 non-null   float64\n",
            " 15  x_blocker_6               1621 non-null   float64\n",
            " 16  x_blocker_7               333 non-null    float64\n",
            " 17  x_blocker_8               0 non-null      float64\n",
            " 18  x_blocker_9               0 non-null      float64\n",
            " 19  y_blocker_1               5000 non-null   float64\n",
            " 20  y_blocker_2               5000 non-null   float64\n",
            " 21  y_blocker_3               5000 non-null   float64\n",
            " 22  y_blocker_4               5000 non-null   float64\n",
            " 23  y_blocker_5               5000 non-null   float64\n",
            " 24  y_blocker_6               1621 non-null   float64\n",
            " 25  y_blocker_7               333 non-null    float64\n",
            " 26  y_blocker_8               0 non-null      float64\n",
            " 27  y_blocker_9               0 non-null      float64\n",
            " 28  speed_qb                  5000 non-null   float64\n",
            " 29  x_qb                      5000 non-null   float64\n",
            " 30  y_qb                      5000 non-null   float64\n",
            " 31  x_ball                    5000 non-null   float64\n",
            " 32  y_ball                    5000 non-null   float64\n",
            " 33  receiver_sep_1            5000 non-null   float64\n",
            " 34  receiver_sep_2            5000 non-null   float64\n",
            " 35  receiver_sep_3            5000 non-null   float64\n",
            " 36  receiver_sep_4            4667 non-null   float64\n",
            " 37  receiver_sep_5            3379 non-null   float64\n",
            " 38  quarter                   5000 non-null   int64  \n",
            " 39  down                      5000 non-null   int64  \n",
            " 40  yards_to_go               5000 non-null   int64  \n",
            " 41  absolute_yardline_number  5000 non-null   float64\n",
            " 42  score_delta               5000 non-null   int64  \n",
            "dtypes: float64(34), int64(9)\n",
            "memory usage: 1.7 MB\n"
          ]
        }
      ],
      "source": [
        "# examine missingness, models will need balanced sequences\n",
        "df.info()\n",
        "# get coords of 5 linemen\n",
        "# nblockers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "v5mAgPj_XK_u"
      },
      "outputs": [],
      "source": [
        "def make_features(df: pd.DataFrame, group_by: list, feats: list, outcomes: list, naive = False):\n",
        "  # check this\n",
        "  X, y  = [], []\n",
        "  grouped_df = df.groupby(group_by)\n",
        "  for _, group_df in grouped_df:\n",
        "    if naive:\n",
        "      X.append(group_df[feats].to_numpy())\n",
        "      y.append(group_df[outcomes])\n",
        "    else:\n",
        "      X.append(group_df[feats].fillna(-99.).to_numpy())\n",
        "      y.append(group_df[outcomes].to_numpy()[0])\n",
        "  \n",
        "  # let's try two things for sensitivity:\n",
        "  # chunk the test set, so we have various 1:5, 1:2, 1:11...\n",
        "  # chunk the  training as well (to see if we actually have to train them on chunks)\n",
        "  # oversampling\n",
        "\n",
        "  if naive:\n",
        "    X = np.concatenate(X)\n",
        "    indices = pd.isnull(X).any(axis=0)\n",
        "    X = np.delete(X, indices, axis=1)\n",
        "    y = np.concatenate(y).ravel()\n",
        "  else:\n",
        "    X = tf.keras.utils.pad_sequences(X, dtype=\"float\", padding=\"pre\", value = -99)\n",
        "    y = tf.keras.utils.pad_sequences(y, dtype=\"float\", padding=\"pre\", value= -99)\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ReSaP: **Re**current **Sa**ck **P**robabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_metrics = {}\n",
        "def add_metrics(model, outcome_name: str, model_name: str, X_test: np.ndarray, y_test: np.ndarray):\n",
        "    global model_metrics\n",
        "    model_metrics[outcome_name] = {}\n",
        "    y_pred = model.predict(X_test)\n",
        "    if type(model) == LogisticRegression:\n",
        "        y_score = model.decision_function(X_test)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_score, pos_label = 1)\n",
        "        roc = RocCurveDisplay(fpr = fpr, tpr = tpr)\n",
        "        prec, recall, _ = precision_recall_curve(y_test, y_score, pos_label= 1)\n",
        "        pr = PrecisionRecallDisplay(precision=prec, recall=recall)\n",
        "        auc = roc_auc_score(y_test, y_score)\n",
        "        model_metrics[outcome_name][model_name] = {\"auc\": auc, \"roc_curve\": roc, \"pr_curve\": pr}\n",
        "    else:\n",
        "        evaluation = model.evaluate(X_test, y_test)\n",
        "        auc = evaluation[1]\n",
        "        acc = evaluation[2]\n",
        "        model_metrics[outcome_name][model_name] = {\"auc\": auc, \"acc\": acc}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \"No Sack\" Pediction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xFLw3CeFvYJv"
      },
      "source": [
        "### \"Naive\" Logistic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "outcomes = [\"pff_sack\", \"havoc\"]\n",
        "datasets = {}\n",
        "for o in outcomes:\n",
        "    group_by = [\"game_id\", \"play_id\", \"nfl_id\"]\n",
        "    outcome = [o]\n",
        "    feats = [x for x in list(df.columns) if x not in group_by + outcomes]\n",
        "\n",
        "    X, y = make_features(df, group_by, feats, outcome, naive = True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
        "\n",
        "    weight_0 = (1 / sum(y_train == [0])) * (len(y_train) / 2.0)\n",
        "    weight_1 = (1 / sum(y_train == [1])) * (len(y_train) / 2.0)\n",
        "    class_weight = {0: weight_0, 1: weight_1}\n",
        "\n",
        "    datasets[o] = {\"X_train\": X_train, \"X_test\": X_test,\n",
        "    \"y_train\": y_train, \"y_test\": y_test, \"class_weight\": class_weight}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJbARXxAvVz4",
        "outputId": "ba916dc3-8f10-4134-d1d5-f04511c91fde"
      },
      "outputs": [],
      "source": [
        "for k, v in datasets.items():\n",
        "    log_reg = LogisticRegression(class_weight = v[\"class_weight\"], max_iter = 1000)\n",
        "    log_reg.fit(v[\"X_train\"], v[\"y_train\"])\n",
        "    add_metrics(log_reg, k, \"logistic\", v[\"X_test\"], v[\"y_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'pff_sack': {'logistic': {'auc': 0.9106793294280415,\n",
              "   'roc_curve': <sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f860eb3e4d0>,\n",
              "   'pr_curve': <sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f8760ae5cf0>}},\n",
              " 'havoc': {'logistic': {'auc': 0.7517986627742727,\n",
              "   'roc_curve': <sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f8495295570>,\n",
              "   'pr_curve': <sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f848cbfe5c0>}}}"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nti1gLWph2gp"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outcomes = [\"pff_sack\", \"havoc\"]\n",
        "datasets = {}\n",
        "for o in outcomes:\n",
        "    group_by = [\"game_id\", \"play_id\", \"nfl_id\"]\n",
        "    outcome = [o]\n",
        "    feats = [x for x in list(df.columns) if x not in group_by + outcomes]\n",
        "\n",
        "    X, y = make_features(df, group_by, feats, outcome)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
        "\n",
        "    weight_0 = (1 / sum(y_train == [0])) * (len(y_train) / 2.0)\n",
        "    weight_1 = (1 / sum(y_train == [1])) * (len(y_train) / 2.0)\n",
        "    class_weight = {0: weight_0, 1: weight_1}\n",
        "\n",
        "    num_epochs = 1\n",
        "    val = 0.2 \n",
        "\n",
        "    datasets[o] = {\"X_train\": X_train, \"X_test\": X_test,\n",
        "    \"y_train\": y_train, \"y_test\": y_test, \"class_weight\": class_weight,\n",
        "    \"num_epochs\": num_epochs, \"val\": val}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcQVj0YNcu8I",
        "outputId": "ca20007e-fae5-4f1c-a69f-e61275600477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "688/688 [==============================] - 216s 296ms/step - loss: 0.6973 - auc_2: 0.5931 - binary_accuracy: 0.8039 - val_loss: 0.6931 - val_auc_2: 0.8461 - val_binary_accuracy: 0.5128\n",
            "Epoch 2/50\n",
            "327/688 [=============>................] - ETA: 1:38 - loss: 0.5740 - auc_2: 0.8478 - binary_accuracy: 0.7922"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[74], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m lstm_mod\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m  metrics \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mAUC(), tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mBinaryAccuracy()])\n\u001b[1;32m     14\u001b[0m callbacks \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(patience \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)]\n\u001b[0;32m---> 16\u001b[0m lstm_mod\u001b[39m.\u001b[39;49mfit(v[\u001b[39m\"\u001b[39;49m\u001b[39mX_train\u001b[39;49m\u001b[39m\"\u001b[39;49m], v[\u001b[39m\"\u001b[39;49m\u001b[39my_train\u001b[39;49m\u001b[39m\"\u001b[39;49m], epochs \u001b[39m=\u001b[39;49m v[\u001b[39m\"\u001b[39;49m\u001b[39mnum_epochs\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[1;32m     17\u001b[0m validation_split \u001b[39m=\u001b[39;49m v[\u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m], class_weight \u001b[39m=\u001b[39;49m v[\u001b[39m\"\u001b[39;49m\u001b[39mclass_weight\u001b[39;49m\u001b[39m\"\u001b[39;49m], callbacks \u001b[39m=\u001b[39;49m callbacks)\n\u001b[1;32m     19\u001b[0m add_metrics(lstm_mod, k, \u001b[39m\"\u001b[39m\u001b[39mlstm\u001b[39m\u001b[39m\"\u001b[39m, v[\u001b[39m\"\u001b[39m\u001b[39mX_test\u001b[39m\u001b[39m\"\u001b[39m], v[\u001b[39m\"\u001b[39m\u001b[39my_test\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m~/Documents/nfl-big-data-bowl-2023/.conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for k, v in datasets.items():\n",
        "\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    lstm_mod = tf.keras.Sequential()\n",
        "    lstm_mod.add(tf.keras.layers.Masking(mask_value= -99., input_shape= v[\"X_train\"].shape[1:]))\n",
        "    lstm_mod.add(tf.keras.layers.LSTM(128, input_shape = v[\"X_train\"].shape[1:]))\n",
        "    lstm_mod.add(tf.keras.layers.Dense(v[\"y_train\"].shape[1], activation=\"sigmoid\"))\n",
        "\n",
        "    lstm_mod.compile(loss = \"binary_crossentropy\", optimizer=\"adam\",\n",
        "     metrics = [tf.keras.metrics.AUC(), tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(patience = 5, restore_best_weights=True)]\n",
        "\n",
        "    lstm_mod.fit(v[\"X_train\"], v[\"y_train\"], epochs = v[\"num_epochs\"], \n",
        "    validation_split = v[\"val\"], class_weight = v[\"class_weight\"], callbacks = callbacks)\n",
        "    \n",
        "    add_metrics(lstm_mod, k, \"lstm\", v[\"X_test\"], v[\"y_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# roc curve\n",
        "# accuracy: predict no sack\n",
        "# logistic \n",
        "# lstm\n",
        "# break test into chunks\n",
        "model_metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7_EiKFYE9qgi"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "YRjBrCEW-8VU"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    lstm_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    x = tf.keras.layers.Masking(mask_value=-99.,input_shape= input_shape)(x)\n",
        "    x = tf.keras.layers.LSTM(lstm_units, input_shape = input_shape, return_sequences=True)(x)\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "    \n",
        "    for dim in mlp_units:\n",
        "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
        "    \n",
        "    outputs = tf.keras.layers.Dense(y.shape[1], activation=\"sigmoid\")(x)\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "outcomes = [\"pff_sack\", \"havoc\"]\n",
        "datasets = {}\n",
        "for o in outcomes:\n",
        "    group_by = [\"game_id\", \"play_id\", \"nfl_id\"]\n",
        "    outcome = [o]\n",
        "    feats = [x for x in list(df.columns) if x not in group_by + outcomes]\n",
        "\n",
        "    X, y = make_features(df, group_by, feats, outcome)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
        "\n",
        "    weight_0 = (1 / sum(y_train == [0])) * (len(y_train) / 2.0)\n",
        "    weight_1 = (1 / sum(y_train == [1])) * (len(y_train) / 2.0)\n",
        "    class_weight = {0: weight_0, 1: weight_1}\n",
        "\n",
        "    num_epochs = 1\n",
        "    val = 0.2 \n",
        "\n",
        "    datasets[o] = {\"X_train\": X_train, \"X_test\": X_test,\n",
        "    \"y_train\": y_train, \"y_test\": y_test, \"class_weight\": class_weight,\n",
        "    \"num_epochs\": num_epochs, \"val\": val}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 35s 1s/step - loss: 0.7943 - auc_5: 0.2704 - val_loss: 0.4314 - val_auc_5: 0.7911\n",
            "7/7 [==============================] - 10s 389ms/step - loss: 0.4180 - auc_5: 0.2487\n",
            "20/20 [==============================] - 41s 1s/step - loss: 0.7138 - auc_6: 0.4563 - val_loss: 0.7314 - val_auc_6: 0.6400\n",
            "7/7 [==============================] - 6s 329ms/step - loss: 0.7361 - auc_6: 0.5833\n"
          ]
        }
      ],
      "source": [
        "for k, v in datasets.items():\n",
        "    input_shape = v[\"X_train\"].shape[1:]\n",
        "\n",
        "    model = build_model(\n",
        "        input_shape,\n",
        "        head_size=128,\n",
        "        num_heads=4,\n",
        "        ff_dim=4,\n",
        "        num_transformer_blocks=1,\n",
        "        mlp_units=[128],\n",
        "        mlp_dropout=0.2,\n",
        "        dropout=0.25,\n",
        "        lstm_units=32\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        #optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        metrics = [tf.keras.metrics.AUC()]\n",
        "    )\n",
        "    #model.summary()\n",
        "\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(min_delta=0.01, patience=3, restore_best_weights=True)]\n",
        "\n",
        "    model.fit(\n",
        "        v[\"X_train\"],\n",
        "        v[\"y_train\"],\n",
        "        validation_split=v[\"val\"],\n",
        "        epochs=v[\"num_epochs\"],\n",
        "        #batch_size=64,\n",
        "        callbacks=callbacks,\n",
        "        class_weight = v[\"class_weight\"]\n",
        "    )\n",
        "\n",
        "    model.evaluate(v[\"X_test\"], v[\"y_test\"], verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "84233dc2148a4ba24349c930a847c50600752012508169725523fba31fb5cdf9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
